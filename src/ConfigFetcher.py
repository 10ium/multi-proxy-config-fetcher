import re
import os
import time
import json
import logging 
import socket 
import requests
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Set, Tuple, Any 

import concurrent.futures
import threading
from collections import defaultdict 

# **ØªØºÛŒÛŒØ± ÛŒØ§ÙØªÙ‡**: ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ù¾ÛŒØ´ÙˆÙ†Ø¯ 'src.'
from config import ProxyConfig, ChannelConfig
from config_validator import ConfigValidator
from source_fetcher import SourceFetcher 
from config_processor import ConfigProcessor 
from deduplicator import Deduplicator 
from connection_tester import ConnectionTester 
from output_manager import OutputManager 
from config_filter import ConfigFilter 
from user_settings import SOURCE_URLS, SPECIFIC_CONFIG_COUNT, BLOCKED_KEYWORDS, BLOCKED_IPS, BLOCKED_DOMAINS 
from user_settings import ALLOWED_COUNTRIES, BLOCKED_COUNTRIES, ALLOWED_PROTOCOLS 

# Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ (Ø§ÛŒÙ† Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¯Ø± main.py Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯)
logger = logging.getLogger(__name__)

class ConfigFetcher:
    """
    Ú©Ù„Ø§Ø³ ConfigFetcher Ù…Ø³Ø¦ÙˆÙ„ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ú©Ù„ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ ÙˆØ§Ú©Ø´ÛŒØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø§Ú©Ø³ÛŒ Ø§Ø³Øª.
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø§Ú©Ù†ÙˆÙ† Pipeline Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    def __init__(self, config: ProxyConfig):
        """
        Ø³Ø§Ø²Ù†Ø¯Ù‡ Ú©Ù„Ø§Ø³ ConfigFetcher.
        """
        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ConfigFetcher...")
        self.config = config
        self.validator = ConfigValidator()
        self.source_fetcher = SourceFetcher(config, self.validator) 
        self.deduplicator = Deduplicator() 
        self.config_processor = ConfigProcessor(config, self.validator) 
        self.connection_tester = ConnectionTester(config, self.validator, self.get_location) 
        self.output_manager = OutputManager(config) 
        self.config_filter = ConfigFilter(config, self.validator) 
        
        self.protocol_counts: Dict[str, int] = defaultdict(int) 
        self.channel_protocol_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int)) 
        
        self.ip_location_cache: Dict[str, Tuple[str, str]] = {} 
        self._lock = threading.Lock() 

        self.retry_intervals = [
            timedelta(days=0),
            timedelta(days=3),
            timedelta(weeks=1),
            timedelta(days=30),
            timedelta(days=90),
            timedelta(days=240)
        ]
        self.max_retry_level = len(self.retry_intervals) - 1 

        self.initial_user_settings_urls: Set[str] = {self.config._normalize_url(url) for url in SOURCE_URLS}
        self.previous_stats_urls: Set[str] = set()
        self._load_previous_stats_urls()

        logger.info("Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ConfigFetcher Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

    def _load_previous_stats_urls(self):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ URLÙ‡Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„ Ø§Ø² channel_stats.json Ù‚Ø¨Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯.
        """
        stats_file_path = os.path.join(self.config.OUTPUT_DIR, 'channel_stats.json')
        if os.path.exists(stats_file_path):
            try:
                with open(stats_file_path, 'r', encoding='utf-8') as f:
                    stats_data = json.load(f)
                for channel_data in stats_data.get('channels', []):
                    try:
                        self.previous_stats_urls.add(self.config._normalize_url(channel_data['url']))
                    except ValueError as e:
                        logger.warning(f"URL Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¯Ø± stats.json Ù‚Ø¨Ù„ÛŒ ÛŒØ§ÙØª Ø´Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {channel_data.get('url', 'Ù†Ø§Ù…Ø¹Ù„ÙˆÙ…')} - {str(e)}")
                logger.debug(f"{len(self.previous_stats_urls)} URL Ø§Ø² stats.json Ù‚Ø¨Ù„ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
            except Exception as e:
                logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ URLÙ‡Ø§ Ø§Ø² stats.json Ù‚Ø¨Ù„ÛŒ: {str(e)}")

    def _get_location_from_ip_api(self, ip: str) -> Tuple[str, str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ø§Ø² ip-api.com"""
        try:
            response = self.source_fetcher.session.get(f'http://ip-api.com/json/{ip}', timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 'success' and data.get('countryCode'):
                    return data['countryCode'].lower(), data['country']
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± API ip-api.com Ø¨Ø±Ø§ÛŒ IP {ip}: {str(e)}")
        return '', ''

    def _get_location_from_ipapi_co(self, ip: str) -> Tuple[str, str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ø§Ø² ipapi.co"""
        try:
            response = self.source_fetcher.session.get(f'https://ipapi.co/{ip}/json/', timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('country_code') and data.get('country_name'):
                    return data['country_code'].lower(), data['country_name']
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± API ipapi.co Ø¨Ø±Ø§ÛŒ IP {ip}: {str(e)}")
        return '', ''

    def _get_location_from_ipwhois(self, ip: str) -> Tuple[str, str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ø§Ø² ipwhois.app"""
        try:
            response = self.source_fetcher.session.get(f'https://ipwhois.app/json/{ip}', timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('country_code') and data.get('country'):
                    return data['country_code'].lower(), data['country']
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± API ipwhois.app Ø¨Ø±Ø§ÛŒ IP {ip}: {str(e)}")
        return '', ''

    def _get_location_from_ipdata(self, ip: str) -> Tuple[str, str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ø§Ø² api.ipdata.co (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ú©Ù„ÛŒØ¯ API ÙˆØ§Ù‚Ø¹ÛŒ Ø¯Ø§Ø±Ø¯)"""
        try:
            response = self.source_fetcher.session.get(f'https://api.ipdata.co/{ip}?api-key=test', timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('country_code') and data.get('country_name'):
                    return data['country_code'].lower(), data['country']
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± API ipdata.co Ø¨Ø±Ø§ÛŒ IP {ip}: {str(e)}")
        return '', ''

    def _get_location_from_abstractapi(self, ip: str) -> Tuple[str, str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ø§Ø² ipgeolocation.abstractapi.com (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ú©Ù„ÛŒØ¯ API ÙˆØ§Ù‚Ø¹ÛŒ Ø¯Ø§Ø±Ø¯)"""
        try:
            response = self.source_fetcher.session.get(f'https://ipgeolocation.abstractapi.com/v1/?api_key=test', timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('country_code') and data.get('country'):
                    return data['country_code'].lower(), data['country']
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± API abstractapi.com Ø¨Ø±Ø§ÛŒ IP {ip}: {str(e)}")
        return '', ''

    def get_location(self, address: str) -> Tuple[str, str]:
        """
        Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ (Ù¾Ø±Ú†Ù… Ùˆ Ù†Ø§Ù… Ú©Ø´ÙˆØ±) Ø±Ø§ Ø§Ø² ÛŒÚ© Ø¢Ø¯Ø±Ø³ (Ø¯Ø§Ù…Ù†Ù‡/IP) Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        Ø§Ø² Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        if address == "162.159.192.1": # Cloudflare Anycast IP
             logger.debug(f"Ø¢Ø¯Ø±Ø³ '{address}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Cloudflare Anycast Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÙˆÙ‚Ø¹ÛŒØª Ù¾ÛŒØ´â€ŒÙØ±Ø¶.")
             return "ğŸ‡ºğŸ‡¸", "Cloudflare"

        try:
            ip = socket.gethostbyname(address)

            with self._lock: 
                if ip in self.ip_location_cache:
                    logger.debug(f"Ù…ÙˆÙ‚Ø¹ÛŒØª IP '{ip}' Ø§Ø² Ú©Ø´ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯.")
                    return self.ip_location_cache[ip]

            apis = [
                self._get_location_from_ip_api,
                self._get_location_from_ipapi_co,
                self._get_location_from_ipwhois,
                self._get_location_from_ipdata,
                self._get_location_from_abstractapi
            ]

            for api_func in apis:
                country_code, country = api_func(ip)
                if country_code and country and len(country_code) == 2:
                    flag = ''.join(chr(ord('ğŸ‡¦') + ord(c.upper()) - ord('A')) for c in country_code)
                    with self._lock: 
                        self.ip_location_cache[ip] = (flag, country)
                    logger.debug(f"Ù…ÙˆÙ‚Ø¹ÛŒØª IP '{ip}' Ø§Ø² API {api_func.__name__} Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯: {flag} {country}")
                    return flag, country

        except socket.gaierror:
            logger.debug(f"Ù†Ø§Ù… Ù…ÛŒØ²Ø¨Ø§Ù† Ù‚Ø§Ø¨Ù„ Ø­Ù„ Ù†ÛŒØ³Øª: '{address}'. Ù…ÙˆÙ‚Ø¹ÛŒØª 'Ù†Ø§Ù…Ø´Ø®Øµ' Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.") 
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¨Ø±Ø§ÛŒ '{address}': {str(e)}")

        with self._lock: 
            self.ip_location_cache[address] = ("ğŸ³ï¸", "Unknown") 
        return "ğŸ³ï¸", "Unknown"


    def add_new_telegram_channel(self, new_channel_url: str):
        """
        ÛŒÚ© Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù… Ø¬Ø¯ÛŒØ¯ Ø±Ø§ (Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯) Ø¨Ù‡ Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ "bot" Ø®ØªÙ… Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
        """
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø§Ø² URL
        channel_name_match = re.search(r't\.me/(?:s/)?([a-zA-Z0-9_]+)', new_channel_url)
        channel_name = channel_name_match.group(1) if channel_name_match else None

        if channel_name and channel_name.lower().endswith('bot'):
            logger.debug(f"URL ØªÙ„Ú¯Ø±Ø§Ù… Ø¨Ù‡ ÛŒÚ© Ø±Ø¨Ø§Øª Ø®ØªÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯: '{new_channel_url}'. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
            return # Ø§Ú¯Ø± Ø¨Ø§Øª Ø¨ÙˆØ¯ØŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†

        is_new_channel = True
        with self._lock: # Ù…Ø­Ø§ÙØ¸Øª Ø§Ø² Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ self.config.SOURCE_URLS
            for existing_channel in self.config.SOURCE_URLS:
                if self.config._normalize_url(existing_channel.url) == self.config._normalize_url(new_channel_url):
                    is_new_channel = False
                    break

            if is_new_channel:
                try:
                    new_channel_config = ChannelConfig(url=new_channel_url)
                    self.config.SOURCE_URLS.append(new_channel_config)
                    logger.info(f"Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù… Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÙˆÛŒØ§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: '{new_channel_url}'.")
                except ValueError as e:
                    logger.warning(f"URL Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù… Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ù¾ÛŒØ¯Ø§ Ø´Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: '{new_channel_url}' - {e}")


    def _fetch_raw_data_for_channel(self, channel: ChannelConfig) -> Tuple[List[str], List[str], Dict[str, Any]]:
        """
        ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… (Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯) Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„ Ø¬Ø¯ÛŒØ¯ Ø§Ø² ÛŒÚ© Ú©Ø§Ù†Ø§Ù„.
        Ø§ÛŒÙ† Ù…ØªØ¯ Ø§Ú©Ù†ÙˆÙ† ÙˆØ§Ú©Ø´ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ø¨Ù‡ SourceFetcher Ù…Ø­ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        return self.source_fetcher.fetch_channel_data(channel)

    def _select_batch_for_testing(self, 
                                  unique_processed_configs_pool: List[Dict[str, str]], 
                                  tested_protocol_counts: Dict[str, int],
                                  batch_size_multiplier: int = 3) -> List[Dict[str, str]]:
        """
        ÛŒÚ© Ø¯Ø³ØªÙ‡ Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ Ù‡Ù†ÙˆØ² Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø®ÙˆØ¯ Ù†Ø±Ø³ÛŒØ¯Ù‡â€ŒØ§Ù†Ø¯.
        batch_size_multiplier: Ú†Ù†Ø¯ Ø¨Ø±Ø§Ø¨Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¯Ø± Ù‡Ø± Ù¾Ø±ÙˆØªÚ©Ù„ØŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆØ¯.
        """
        selected_batch: List[Dict[str, str]] = []
        configs_by_protocol: Dict[str, List[Dict[str, str]]] = defaultdict(list)
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±ÙˆØªÚ©Ù„
        for cfg in unique_processed_configs_pool:
            configs_by_protocol[cfg['protocol']].append(cfg)

        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÙˆÙ„ÙˆÛŒØª Ùˆ Ù†ÛŒØ§Ø² (Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù…ØªØ±ÛŒ Ø±Ø³ÛŒØ¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯)
        # Ùˆ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ± (Ø¹Ø¯Ø¯ Ú©Ù…ØªØ±)
        sorted_protocols_by_priority_and_need = sorted(
            self.config.SUPPORTED_PROTOCOLS.items(),
            key=lambda item: (
                tested_protocol_counts.get(item[0], 0), # Ú©Ù…ØªØ± ØªØ³Øª Ø´Ø¯Ù‡â€ŒÙ‡Ø§ Ø§ÙˆÙ„
                item[1]["priority"] # Ø³Ù¾Ø³ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ±
            )
        )

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§ÛŒØ² Ù‡Ø¯Ù Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¯Ø³ØªÙ‡ØŒ Ø­Ø¯Ø§Ù‚Ù„ 100 ÛŒØ§ (ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ * ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø´Ø®Øµ)
        target_total_batch_size = max(self.config.specific_config_count * len([p for p in self.config.SUPPORTED_PROTOCOLS if self.config.SUPPORTED_PROTOCOLS[p]['enabled']]), 100) 
        current_batch_size = 0
        
        for protocol_prefix, protocol_info in sorted_protocols_by_priority_and_need:
            # Ø§Ú¯Ø± Ù¾Ø±ÙˆØªÚ©Ù„ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª ÛŒØ§ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒ Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³ØªØŒ Ø±Ø¯ Ú©Ù†
            if not protocol_info["enabled"] or \
               tested_protocol_counts.get(protocol_prefix, 0) >= protocol_info["max_configs"]:
                continue

            needed_for_protocol = protocol_info["max_configs"] - tested_protocol_counts.get(protocol_prefix, 0)
            if needed_for_protocol <= 0: # Ø§Ú¯Ø± Ù¾Ø±ÙˆØªÚ©Ù„ Ø¨Ù‡ Ø­Ø¯ Ù†ØµØ§Ø¨ Ø±Ø³ÛŒØ¯Ù‡ Ø¨Ø§Ø´Ø¯
                continue

            available_for_protocol = configs_by_protocol[protocol_prefix]
            
            # ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø§Ø² Ø§ÛŒÙ† Ù¾Ø±ÙˆØªÚ©Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒÙ…
            num_to_select = min(len(available_for_protocol), needed_for_protocol * batch_size_multiplier)
            
            selected_batch.extend(available_for_protocol[:num_to_select])
            
            current_batch_size += num_to_select
            if current_batch_size >= target_total_batch_size and len(selected_batch) > 0:
                break 
        
        selected_ids = {cfg['canonical_id'] for cfg in selected_batch}
        unique_processed_configs_pool[:] = [cfg for cfg in unique_processed_configs_pool if cfg['canonical_id'] not in selected_ids]


        logger.info(f"ÛŒÚ© Ø¯Ø³ØªÙ‡ {len(selected_batch)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯. {len(unique_processed_configs_pool)} Ú©Ø§Ù†ÙÛŒÚ¯ ØªØ³Øªâ€ŒÙ†Ø´Ø¯Ù‡ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯.")
        return selected_batch


    def balance_protocols(self, configs: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±ÙˆØªÚ©Ù„ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ùˆ Ù…ØªØ¹Ø§Ø¯Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ ØªÙˆØ²ÛŒØ¹ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯.
        Ø§ÛŒÙ† Ù…ØªØ¯ ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ø±ÙˆØªÚ©Ù„ Ø§Ø² "max_configs" ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡
        Ø¯Ø± ØªÙ†Ø¸ÛŒÙ…Ø§Øª (Ø¨Ø±Ø§ÛŒ Ø¢Ù† Ù¾Ø±ÙˆØªÚ©Ù„) ØªØ¬Ø§ÙˆØ² Ù†Ú©Ù†Ø¯.
        """
        logger.info("Ø´Ø±ÙˆØ¹ ØªÙˆØ§Ø²Ù† Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§...")
        protocol_configs: Dict[str, List[Dict[str, str]]] = defaultdict(list)
        for config_dict in configs:
            protocol = config_dict['protocol']
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªØ¹Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø²Ù†
            if protocol.startswith('hy2://'):
                protocol = 'hysteria2://'
            elif protocol.startswith('hy1://'):
                protocol = 'hysteria://'

            if protocol in self.config.SUPPORTED_PROTOCOLS: # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù¾Ø±ÙˆØªÚ©Ù„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                protocol_configs[protocol].append(config_dict)
            else:
                logger.warning(f"Ù¾Ø±ÙˆØªÚ©Ù„ '{protocol}' Ø¯Ø± Ù„ÛŒØ³Øª Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø²Ù† ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.")

        total_configs = sum(len(configs_list) for configs_list in protocol_configs.values())
        if total_configs == 0:
            logger.info("Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø²Ù† Ù¾Ø±ÙˆØªÚ©Ù„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
            return []

        balanced_configs: List[Dict[str, str]] = []
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÙˆÙ„ÙˆÛŒØª Ùˆ Ø³Ù¾Ø³ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        sorted_protocols = sorted(
            protocol_configs.items(),
            key=lambda x: (
                self.config.SUPPORTED_PROTOCOLS.get(x[0], {"priority": 999})["priority"], 
                len(x[1])
            ),
            reverse=True # Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ± (Ø¹Ø¯Ø¯ Ú©Ù…ØªØ±) Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø¨ÛŒØ´ØªØ±ØŒ Ø²ÙˆØ¯ØªØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯
        )
        logger.info(f"Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ§Ø²Ù† {total_configs} Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ {len(sorted_protocols)} Ù¾Ø±ÙˆØªÚ©Ù„ Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡...")

        for protocol, protocol_config_list in sorted_protocols:
            protocol_info = self.config.SUPPORTED_PROTOCOLS.get(protocol)
            if not protocol_info:
                logger.warning(f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„ '{protocol}' ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                continue

            # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„ Ø¨ÛŒØ´ØªØ± ÛŒØ§ Ù…Ø³Ø§ÙˆÛŒ Ø­Ø¯Ø§Ù‚Ù„ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯
            if len(protocol_config_list) >= protocol_info["min_configs"]:
                # ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ø¬Ø§Ø² ÛŒØ§ Ù…ÙˆØ¬ÙˆØ¯ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                num_to_add = min(
                    protocol_info["max_configs"],  # Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ø¬Ø§Ø² ØªÙˆØ³Ø· ØªÙ†Ø¸ÛŒÙ…Ø§Øª
                    len(protocol_config_list)     # ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ù…ÙˆØ¬ÙˆØ¯
                )
                balanced_configs.extend(protocol_config_list[:num_to_add])
                logger.info(f"Ù¾Ø±ÙˆØªÚ©Ù„ '{protocol}': {num_to_add} Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ (Ø§Ø² {len(protocol_config_list)} Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ø¬Ø§Ø²: {protocol_info['max_configs']}).")
            # Ø§Ú¯Ø± Ø­Ø§Ù„Øª flexible_max ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ø¯ Ùˆ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            elif protocol_info["flexible_max"] and len(protocol_config_list) > 0:
                balanced_configs.extend(protocol_config_list)
                logger.info(f"Ù¾Ø±ÙˆØªÚ©Ù„ '{protocol}': {len(protocol_config_list)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ (Ø­Ø§Ù„Øª flexible_max).")
            else:
                logger.debug(f"Ù¾Ø±ÙˆØªÚ©Ù„ '{protocol}': ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ú©Ø§ÙÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ ({len(protocol_config_list)}).")

        logger.info(f"ØªÙˆØ§Ø²Ù† Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(balanced_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù†Ù‡Ø§ÛŒÛŒ.")
        return balanced_configs


    def run_full_pipeline(self):
        """
        Ù…ØªØ¯ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ pipeline ÙˆØ§Ú©Ø´ÛŒØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ØŒ ØªÙˆØ§Ø²Ù† Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§.
        """
        all_raw_configs_collected: List[str] = []
        all_new_channel_urls_discovered: Set[str] = set()

        channels_to_process = []
        now = datetime.now(timezone.utc)

        logger.info(f"Ø¯Ø± Ø­Ø§Ù„ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´. Ø²Ù…Ø§Ù† ÙØ¹Ù„ÛŒ: {now.strftime('%Y-%m-%d %H:%M:%S UTC')}.")
        for channel in list(self.config.SOURCE_URLS):
            if not channel.enabled:
                logger.debug(f"Ú©Ø§Ù†Ø§Ù„ '{channel.url}' ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                continue
            if channel.next_check_time and channel.next_check_time > now:
                logger.info(f"Ú©Ø§Ù†Ø§Ù„ '{channel.url}' Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. Ø²Ù…Ø§Ù† Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø¹Ø¯ÛŒ: {channel.next_check_time.strftime('%Y-%m-%d %H:%M:%S UTC')}.")
                continue
            channels_to_process.append(channel)
            logger.debug(f"Ú©Ø§Ù†Ø§Ù„ '{channel.url}' Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯.")

        total_channels_to_process = len(channels_to_process)
        if total_channels_to_process == 0:
            logger.info("Ù‡ÛŒÚ† Ú©Ø§Ù†Ø§Ù„ ÙØ¹Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ (ÛŒØ§ Ù‡Ù…Ù‡ Ø¯Ø± Ø­Ø§Ù„Øª ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‡Ø³ØªÙ†Ø¯). ÙØ±Ø¢ÛŒÙ†Ø¯ ÙˆØ§Ú©Ø´ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
            return []

        logger.info(f"Ø´Ø±ÙˆØ¹ ÙØ§Ø² Û±: ÙˆØ§Ú©Ø´ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ùˆ Ú©Ø´Ù Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø§Ø² {total_channels_to_process} Ú©Ø§Ù†Ø§Ù„ ÙØ¹Ø§Ù„...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=min(10, total_channels_to_process + 1)) as executor:
            futures = {executor.submit(self._fetch_raw_data_for_channel, channel): channel for channel in channels_to_process}

            processed_channels_count = 0
            for future in concurrent.futures.as_completed(futures):
                channel_processed = futures[future]
                processed_channels_count += 1
                progress_percentage = (processed_channels_count / total_channels_to_process) * 100

                try:
                    raw_configs, new_channel_urls, channel_status_info = future.result()

                    channel_processed.metrics.total_configs = channel_status_info['total_configs_raw']

                    if channel_status_info['success']:
                        self.config.update_channel_stats(channel_processed, True, channel_status_info['response_time'])
                        channel_processed.retry_level = 0
                        channel_processed.next_check_time = None
                        logger.info(f"Ù¾ÛŒØ´Ø±ÙØª: {progress_percentage:.2f}% ({processed_channels_count}/{total_channels_to_process}) - Ú©Ø§Ù†Ø§Ù„ '{channel_processed.url}' ÙˆØ§Ú©Ø´ÛŒ Ø´Ø¯. ({len(raw_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù…ØŒ {len(new_channel_urls)} Ú©Ø§Ù†Ø§Ù„ Ø¬Ø¯ÛŒØ¯ Ù¾ÛŒØ¯Ø§ Ø´Ø¯).")
                    else:
                        self.config.update_channel_stats(channel_processed, False, channel_status_info['response_time'])
                        channel_processed.retry_level = min(channel_processed.retry_level + 1, self.max_retry_level)
                        channel_processed.next_check_time = datetime.now(timezone.utc) + self.retry_intervals[channel_processed.retry_level]
                        logger.warning(f"Ù¾ÛŒØ´Ø±ÙØª: {progress_percentage:.2f}% ({processed_channels_count}/{total_channels_to_process}) - Ú©Ø§Ù†Ø§Ù„ '{channel_processed.url}' ÙˆØ§Ú©Ø´ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. Ø®Ø·Ø§: {channel_status_info.get('error_message', 'Ù†Ø§Ù…Ø¹Ù„ÙˆÙ…')}. (Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø¹Ø¯ÛŒ: {channel_processed.next_check_time.strftime('%Y-%m-%d %H:%M:%S UTC')})")

                    all_raw_configs_collected.extend(raw_configs)
                    for url in new_channel_urls:
                        all_new_channel_urls_discovered.add(url) 

                except Exception as exc:
                    logger.error(f"Ù¾ÛŒØ´Ø±ÙØª: {progress_percentage:.2f}% ({processed_channels_count}/{total_channels_to_process}) - Ú©Ø§Ù†Ø§Ù„ '{channel_processed.url}' Ø¯Ø± Ø­ÛŒÙ† ÙˆØ§Ú©Ø´ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {exc}", exc_info=True)

        logger.info(f"ÙØ§Ø² Û± ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(all_raw_configs_collected)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù… Ùˆ {len(all_new_channel_urls_discovered)} URL Ú©Ø§Ù†Ø§Ù„ Ø¬Ø¯ÛŒØ¯ Ú©Ø´Ù Ø´Ø¯.")

        logger.info("Ø´Ø±ÙˆØ¹ ÙØ§Ø² Û²: Ú©Ø´Ù Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø§Ø² ØªÙ…Ø§Ù…ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ Ø§ØµÙ„ÛŒ...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures_phase2 = []
            for raw_cfg_string in all_raw_configs_collected:
                futures_phase2.append(executor.submit(self.validator.extract_telegram_channels_from_config, raw_cfg_string))

            for future in concurrent.futures.as_completed(futures_phase2):
                try:
                    discovered_from_config = future.result()
                    for new_url in discovered_from_config:
                        all_new_channel_urls_discovered.add(new_url)
                except Exception as exc:
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†Ø§Ù„ Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù… (ÙØ§Ø² 2): {exc}", exc_info=True)

        for new_url in all_new_channel_urls_discovered:
            self.add_new_telegram_channel(new_url)
        logger.info(f"ÙØ§Ø² Û² ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ú©Ù†ÙˆÙ† Ø´Ø§Ù…Ù„ {len(self.config.SOURCE_URLS)} Ú©Ø§Ù†Ø§Ù„ Ø§Ø³Øª (Ù¾Ø³ Ø§Ø² Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ù…ÙˆØ§Ø±Ø¯ Ø¬Ø¯ÛŒØ¯).")

        logger.info("Ø´Ø±ÙˆØ¹ ÙØ§Ø² Û³: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø­Ø°Ù Ø§ÙˆÙ„ÛŒÙ‡ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ù‡ Ú©Ø§Ù†ÙˆÙ†ÛŒ) Ø§Ø² Ú©Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ...")
        unique_processed_configs_pool: List[Dict[str, str]] = [] 

        if not all_raw_configs_collected:
            logger.info("Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± ÙØ§Ø² Û³ ÛŒØ§ÙØª Ù†Ø´Ø¯. ÙØ§Ø² Û³ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
            logger.info("ÙØ§Ø² Û³ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ 0 Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ùˆ ØºÙ†ÛŒ Ø´Ø¯Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆØ§Ø²Ù†.")
            return [] 

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: 
            futures = {executor.submit(self.config_processor.process_raw_config, cfg_str): cfg_str for cfg_str in all_raw_configs_collected}

            processed_raw_count_phase3 = 0
            for future in concurrent.futures.as_completed(futures):
                processed_raw_count_phase3 += 1
                progress_percentage_phase3 = (processed_raw_count_phase3 / len(all_raw_configs_collected)) * 100

                try:
                    processed_cfg_data = future.result() 
                    if processed_cfg_data:
                        is_unique = self.deduplicator.is_unique_and_add(processed_cfg_data['canonical_id'])
                        if is_unique:
                            unique_processed_configs_pool.append(processed_cfg_data)
                    if processed_raw_count_phase3 % 100 == 0 or processed_raw_count_phase3 == len(all_raw_configs_collected):
                        logger.info(f"Ù¾ÛŒØ´Ø±ÙØª ÙØ§Ø² Û³: {progress_percentage_phase3:.2f}% ({processed_raw_count_phase3}/{len(all_raw_configs_collected)}) Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯. (Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡: {len(unique_processed_configs_pool)})")
                except Exception as exc:
                    original_raw_config = futures[future]
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù… Ø¯Ø± ÙØ§Ø² Û³: '{original_raw_config[:min(len(original_raw_config), 50)]}...': {exc}", exc_info=True)
            
        logger.info(f"ÙØ§Ø² Û³ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(unique_processed_configs_pool)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ (Ø¨Ø¯ÙˆÙ† Ù¾Ø±Ú†Ù…/Ú©Ø´ÙˆØ±) Ø¢Ù…Ø§Ø¯Ù‡ ØªØ³Øª.")

        logger.info("Ø´Ø±ÙˆØ¹ ÙØ§Ø² Û´: ØªØ³Øª ØªØ¯Ø±ÛŒØ¬ÛŒØŒ ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ùˆ ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ (Ù¾Ø±Ú†Ù…/Ú©Ø´ÙˆØ±) Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ...")
        final_tested_and_enriched_configs: List[Dict[str, str]] = []
        tested_protocol_counts: Dict[str, int] = defaultdict(int) 

        max_attempts_per_batch_loop = 5 
        current_attempt = 0

        while unique_processed_configs_pool and \
              any(tested_protocol_counts.get(p, 0) < self.config.SUPPORTED_PROTOCOLS[p]["max_configs"] 
                  for p in self.config.SUPPORTED_PROTOCOLS if self.config.SUPPORTED_PROTOCOLS[p]["enabled"]) and \
              current_attempt < max_attempts_per_batch_loop: 
            
            batch_to_test = self._select_batch_for_testing(unique_processed_configs_pool, tested_protocol_counts)
            
            if not batch_to_test:
                logger.info("Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø¯Ø± Ø¯Ø³ØªÙ‡ ÙØ¹Ù„ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯. Ø§ØªÙ…Ø§Ù… ÙØ§Ø² Û´.")
                current_attempt += 1 
                continue 
            
            current_attempt = 0 

            logger.info(f"Ø´Ø±ÙˆØ¹ ØªØ³Øª Ø¯Ø³ØªÙ‡ Ø¬Ø¯ÛŒØ¯: {len(batch_to_test)} Ú©Ø§Ù†ÙÛŒÚ¯.")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor_test:
                futures_test = {executor_test.submit(self.connection_tester.test_and_enrich_config, cfg_data): cfg_data for cfg_data in batch_to_test}

                processed_batch_count = 0
                for future_test in concurrent.futures.as_completed(futures_test):
                    processed_batch_count += 1
                    try:
                        enriched_config_dict = future_test.result()
                        if enriched_config_dict:
                            filtered_result = self.config_filter.filter_configs(
                                [enriched_config_dict], 
                                allowed_countries=ALLOWED_COUNTRIES,
                                blocked_countries=BLOCKED_COUNTRIES,
                                allowed_protocols=ALLOWED_PROTOCOLS,
                                blocked_keywords=BLOCKED_KEYWORDS,
                                blocked_ips=BLOCKED_IPS,
                                blocked_domains=BLOCKED_DOMAINS
                            )
                            if filtered_result: 
                                final_tested_and_enriched_configs.append(filtered_result[0]) 
                                self.protocol_counts[filtered_result[0]['protocol']] += 1
                                tested_protocol_counts[filtered_result[0]['protocol']] += 1
                                logger.debug(f"Ú©Ø§Ù†ÙÛŒÚ¯ Ù¾Ø±ÙˆØªÚ©Ù„ '{filtered_result[0]['protocol']}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ ÙØ¹Ù„ÛŒ: {tested_protocol_counts[filtered_result[0]['protocol']]}/{self.config.SUPPORTED_PROTOCOLS[filtered_result[0]['protocol']]['max_configs']}")
                            else:
                                logger.debug(f"Ú©Ø§Ù†ÙÛŒÚ¯ '{enriched_config_dict.get('config', '')[:min(len(enriched_config_dict.get('config', '')), 50)]}...' ØªÙˆØ³Ø· ÙÛŒÙ„ØªØ± Ø±Ø¯ Ø´Ø¯.")

                    except Exception as exc_test:
                        original_cfg_data = futures_test[future_test]
                        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯: '{original_cfg_data.get('config', '')[:min(len(original_cfg_data.get('config', '')), 50)]}...': {exc_test}", exc_info=True)
            
            logger.info(f"Ø¯Ø³ØªÙ‡ {len(batch_to_test)} Ú©Ø§Ù†ÙÛŒÚ¯ ØªØ³Øª Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ ØªØ§ Ú©Ù†ÙˆÙ†: {len(final_tested_and_enriched_configs)}.")

        logger.info(f"ÙØ§Ø² Û´ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(final_tested_and_enriched_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ ØªØ³Øª Ø´Ø¯Ù‡ Ùˆ ØºÙ†ÛŒ Ø´Ø¯Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆØ§Ø²Ù†.")

        total_valid_configs_global = len(final_tested_and_enriched_configs)
        for channel in self.config.SOURCE_URLS:
            channel.metrics.valid_configs = 0 
            channel.metrics.unique_configs = 0 
        
        for protocol, count in self.protocol_counts.items():
            if protocol in self.config.SUPPORTED_PROTOCOLS:
                self.config.SUPPORTED_PROTOCOLS[protocol]["actual_count"] = count 
        
        logger.info("Ø´Ø±ÙˆØ¹ ÙØ§Ø² Ûµ: ØªÙˆØ§Ø²Ù† Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§...")
        final_configs_balanced = self.balance_protocols(final_tested_and_enriched_configs)
        logger.info(f"ÙØ§Ø² Ûµ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. {len(final_configs_balanced)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø³ Ø§Ø² ØªÙˆØ§Ø²Ù† Ø¢Ù…Ø§Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡.")

        return final_configs_balanced

